package uk.ac.imperial.lsds.seepmaster.scheduler.memorymanagement;

import java.util.ArrayList;
import java.util.Comparator;
import java.util.HashMap;
import java.util.HashSet;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Set;
import java.util.stream.Stream;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import uk.ac.imperial.lsds.seep.core.DatasetMetadata;
import uk.ac.imperial.lsds.seep.core.DatasetMetadataPackage;
import uk.ac.imperial.lsds.seep.scheduler.ScheduleDescription;
import uk.ac.imperial.lsds.seep.scheduler.Stage;

public class MDFMemoryManagementPolicy implements MemoryManagementPolicy {

	final private Logger LOG = LoggerFactory.getLogger(MDFMemoryManagementPolicy.class);
	
	private ScheduleDescription sd;
	private double dmRatio;
	
	private Map<Integer, Map<Integer, Double>> euId_mdf = new HashMap<>();
	
	private Map<Integer, Integer> stageid_accesses = new HashMap<>();
	private Map<Integer, Long> stageid_size = new HashMap<>(); // proportional to #nodes
	private Map<Integer, Long> stageid_cost = new HashMap<>(); // proportional to #nodes
	private Map<Integer, Double> stageid_ratio_inmem = new HashMap<>(); // proportional to #nodes
	
	private Map<Integer, Map<Integer, Set<Integer>>> euid_stageid_datasets = new HashMap<>();
	private Map<Integer, Map<Integer, Integer>> euid_stageid_currentAccesses = new HashMap<>();
	
	// Metrics
	private long __totalUpdateTime = 0;
	private long __totalRankTime = 0;
		
	public MDFMemoryManagementPolicy(ScheduleDescription sd, double dmRatio) {
		this.sd = sd;
		this.dmRatio = dmRatio;
		computeAccesses(sd);
	}
	
	@Override
	public void updateDatasetsForNode(int euId, DatasetMetadataPackage datasetsMetadata, int stageId) {
		long start = System.currentTimeMillis();
		if(! euid_stageid_currentAccesses.containsKey(euId)) {
			euid_stageid_currentAccesses.put(euId, stageid_accesses);
		}
		
		// Get datasets generated by this stage
		Set<Integer> datasetsOfThisStage = new HashSet<>();
		for(DatasetMetadata dm : datasetsMetadata.newDatasets) {
			datasetsOfThisStage.add(dm.getDatasetId());
		}
		
		markDatasetsAccessed(euId, stageId);
		
		// Store datasets generated by the dataset so that it's easier to evict them in the future
		if(! euid_stageid_datasets.containsKey(euId)){
			euid_stageid_datasets.put(euId, new HashMap<>());
		}
		euid_stageid_datasets.get(euId).put(stageId, datasetsOfThisStage);
		
		// Compute variables for the model
		long sizeOfThisDataset = computeSizeOfDataset(datasetsMetadata.newDatasets);
		long costOfDataset = computeCostOfDataset(datasetsMetadata.newDatasets);
		double percDataInMem = computeRatioDataInMem(datasetsMetadata.newDatasets, sizeOfThisDataset);
		int accesses = stageid_accesses.get(stageId);
		
		// Store variables
		stageid_size.put(stageId, sizeOfThisDataset);
		stageid_cost.put(stageId, costOfDataset);
		stageid_ratio_inmem.put(stageId, percDataInMem);
		
		double IOCostForThisDataset = sizeOfThisDataset * dmRatio;
		double recomputeCost = computeRecomputeCostFor(stageId);
		
		double factor = Math.min(IOCostForThisDataset, recomputeCost);
		
		double precedence = accesses * factor;
		
		if(! euId_mdf.containsKey(euId)) {
			euId_mdf.put(euId, new HashMap<Integer, Double>());
		}
		for(Integer datasetId : datasetsOfThisStage) {
			euId_mdf.get(euId).put(datasetId, precedence);
		}
		long end = System.currentTimeMillis();
		this.__totalUpdateTime = this.__totalUpdateTime + (end - start);
	}
	
	@Override
	public List<Integer> rankDatasetsForNode(int euId, Set<Integer> datasetIds) {
		long start = System.currentTimeMillis();
		List<Integer> rankedDatasets = new ArrayList<>();
		if(! euId_mdf.containsKey(euId)) {
			return rankedDatasets;
		}
		
		// Now we use the datasets that are alive to prune the datasets in the node
		removeEvictedDatasets(euId, datasetIds);
		
		// Evict datasets based on access patterns
		removeFinishedDatasets(euId);
		
		// We get the datasets in the node, after pruning
		Map<Integer, Double> datasetId_timestamp = euId_mdf.get(euId);
		
		Map<Integer, Double> sorted = sortByValue(datasetId_timestamp);
		System.out.println("MDF VALUES");
		for(Double v : sorted.values()) {
			System.out.print(v+" - ");
		}
		System.out.println();
		
		
		// TODO: may break ordering due to keyset returning a set ?
		for(Integer key : sorted.keySet()) {
			rankedDatasets.add(key);
		}
		long end = System.currentTimeMillis();
		this.__totalRankTime = this.__totalRankTime + (end - start);
		return rankedDatasets;
	}
	
	private void markDatasetsAccessed(int euId, int stageId) {
		Map<Integer, Integer> stageid_currentAccesses = euid_stageid_currentAccesses.get(euId);
		Stage justFinishedStage = sd.getStageWithId(stageId);
		Set<Stage> upstream = justFinishedStage.getDependencies();
		for(Stage s : upstream) {
			int uid = s.getStageId();
			int currentAccesses = stageid_currentAccesses.get(uid);
			int newAccesses = currentAccesses - 1;
			stageid_currentAccesses.put(uid, newAccesses);
		}
		euid_stageid_currentAccesses.put(euId, stageid_currentAccesses);
	}
	
	private double computeRecomputeCostFor(int stageId) {
		double recomputeCost = Long.MAX_VALUE;
		Stage s = sd.getStageWithId(stageId);
		Set<Stage> upstream = s.getDependencies();
		if(upstream.size() > 1) {
			LOG.error("upstream of more than 1 when computing recompute cost for stageId: {}" ,stageId);
		}
		if(! upstream.iterator().hasNext()) {
			return recomputeCost; // source stage, cut
		}
		int sid = upstream.iterator().next().getStageId();
		if(! stageid_size.containsKey(sid)) {
			return recomputeCost; // make sure this is not selected, as it does not exist yet
		}
		long size = stageid_size.get(sid);
		long cost = stageid_cost.get(sid);
		
		double percDataInMem = stageid_ratio_inmem.get(sid);
		
		recomputeCost = cost + percDataInMem * (size * dmRatio);
		
		return recomputeCost;
	}
	
	private long computeSizeOfDataset(Set<DatasetMetadata> datasetsMetadata) {
		long size = 0;
		for(DatasetMetadata dm : datasetsMetadata) {
			size = size + dm.getSize();
		}
		return size;
	}
	
	private long computeCostOfDataset(Set<DatasetMetadata> datasetsMetadata) {
		long cost = 0;
		for(DatasetMetadata dm : datasetsMetadata) {
			cost = cost + dm.getCreationCost();
		}
		return cost;
	}
	
	private double computeRatioDataInMem(Set<DatasetMetadata> datasetsMetadata, long sizeOfThisDataset) {
		// doing this on actual size in case datasets are of different lenghts in the future
		double r = 0;
		long mem = 0;
		for(DatasetMetadata dm : datasetsMetadata) {
			if(dm.isInMem()) {
				mem = mem + dm.getSize();
			}
		}
		sizeOfThisDataset++;
		r = mem/sizeOfThisDataset;
		return r;
	}
	
	private void removeFinishedDatasets(int euId) {
		Map<Integer, Integer> stageid_currentAccesses = euid_stageid_currentAccesses.get(euId);
		Map<Integer, Set<Integer>> stageid_datasets = euid_stageid_datasets.get(euId);
		
		// Datasets to evict		
		Set<Integer> toEvict = new HashSet<>();
		
		// Check those datasets that must be evicted
		for(Entry<Integer, Integer> e : stageid_currentAccesses.entrySet()) {
			int sid = e.getKey();
			if(e.getValue() == 0) { // Time to evict
				if(stageid_datasets.containsKey(sid)) {
					Set<Integer> toR = stageid_datasets.get(sid);
					toEvict.addAll(toR);
				}
			}
		}
		
		// Now evict them from the data structure for propagation to the cluster
		for(Entry<Integer, Map<Integer, Double>> e : euId_mdf.entrySet()) {
			Map<Integer, Double> entry = e.getValue(); 
			for(Integer kill : toEvict) {
				entry.remove(kill);
			}
		}
	}
	
	private void computeAccesses(ScheduleDescription sd) {
		// TODO: will work if there is only one (logical) source
		for(Stage s : sd.getStages()) {
			int sid = s.getStageId();
			int numDownstream = s.getDependants().size();
			stageid_accesses.put(sid, numDownstream);
		}
	}
	
	private void removeEvictedDatasets(int euId, Set<Integer> datasetIdsToKeep) {
		Map<Integer, Double> allEntries = euId_mdf.get(euId);
		
		// Select entries to remove
		Set<Integer> toRemove = new HashSet<>();
		for(int id : allEntries.keySet()) {
			if(! datasetIdsToKeep.contains(id)) {
				toRemove.add(id);
			}
		}
		
		// Remove the selection
		for(int toRem : toRemove) {
			allEntries.remove(toRem);
		}
		
		// Update the info
		euId_mdf.put(euId, allEntries);
	}
	
	private Map<Integer, Double> sortByValue( Map<Integer, Double> map ) {
		Map<Integer, Double> result = new LinkedHashMap<>();
		Stream <Entry<Integer, Double>> st = map.entrySet().stream();
		// FIXME: precedence means that higher should be higher in ranked values. Probably need to multiply * -1 down there, or invert the order
		st.sorted(Comparator.comparingDouble(e -> e.getValue())).forEachOrdered(e -> result.put(e.getKey(),e.getValue()));
		return result;
	}

	@Override
	public long __totalUpdateTime() {
		return this.__totalUpdateTime;
	}

	@Override
	public long __totalRankTime() {
		return this.__totalRankTime;
	}

}
